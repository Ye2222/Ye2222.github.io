<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>李宏毅深度学习L3 | Yeの博客</title><meta name="author" content="GuoYB"><meta name="copyright" content="GuoYB"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="卷积神经网络(CNN)，自注意力(Self-attention)机制，循环神经网络(RNN)，Transfomer">
<meta property="og:type" content="article">
<meta property="og:title" content="李宏毅深度学习L3">
<meta property="og:url" content="https://ye2222.github.io/posts/48150/index.html">
<meta property="og:site_name" content="Yeの博客">
<meta property="og:description" content="卷积神经网络(CNN)，自注意力(Self-attention)机制，循环神经网络(RNN)，Transfomer">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ye2222.github.io/img/avatar.png">
<meta property="article:published_time" content="2022-06-01T07:23:07.000Z">
<meta property="article:modified_time" content="2023-04-26T11:19:54.798Z">
<meta property="article:author" content="GuoYB">
<meta property="article:tag" content="DeepLearning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ye2222.github.io/img/avatar.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://ye2222.github.io/posts/48150/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: {"limitDay":100,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: GuoYB","link":"链接: ","source":"来源: Yeの博客","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '李宏毅深度学习L3',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-04-26 19:19:54'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.0.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>const preloader = {
  endLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = 'hidden';
    document.getElementById('loading-box').classList.remove("loaded")
  }
}

preloader.initLoading()
window.addEventListener('load',()=> { preloader.endLoading() })

if (false) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">105</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">9</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">32</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/songs/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> 相册</span></a></li><li><a class="site-page child" href="/books/"><i class="fa-fw fas fa-book"></i><span> 书单</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fas fa-gamepad"></i><span> 游戏</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/default.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Yeの博客"><span class="site-name">Yeの博客</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/songs/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> 相册</span></a></li><li><a class="site-page child" href="/books/"><i class="fa-fw fas fa-book"></i><span> 书单</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fas fa-gamepad"></i><span> 游戏</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">李宏毅深度学习L3</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-06-01T07:23:07.000Z" title="发表于 2022-06-01 15:23:07">2022-06-01</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-04-26T11:19:54.798Z" title="更新于 2023-04-26 19:19:54">2023-04-26</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85/">李宏毅</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="李宏毅深度学习L3"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="convolutional-neural-network-cnn">Convolutional Neural Network
（CNN）</h2>
<ul>
<li>CNN，即卷积神经网络，主要适用于图片处理</li>
</ul>
<h3 id="图片分类">图片分类</h3>
<ul>
<li>假设我们现在有一张彩色的图片，在电脑中它有<strong>红绿蓝</strong>三个通道，每个通道是一个100*100的矩阵</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202205252103858.png"  style="zoom:67%;" /></p>
<ul>
<li>但是对于图片来说，如果我们使用全连接层的模型，参数会变得特别多</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202205252104016.png"  style="zoom:67%;" /></p>
<h3 id="感受野">感受野</h3>
<ul>
<li>我们观察到对于图像分类来说，要抓住的是图像中物体的特征，需要去捕捉图片的<strong>局部信息</strong>
<ul>
<li>如图中鸟的特征：鸟喙、眼睛、鸟爪</li>
</ul></li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202205252104884.png"  style="zoom:67%;" /></p>
<ul>
<li>所以我们设置一个<strong>感受野(Receptive
field)</strong>区域，来提取这一区域覆盖的图片信息(局部信息)，并将信息给予一个神经元
<ul>
<li><strong>kernel size</strong>(感受野或者叫卷积核的大小)：<span
class="math inline">\(n \times n\)</span>，一般为<span
class="math inline">\(3\times3\)</span></li>
<li>区域可以重叠</li>
<li><strong>stride</strong>：移动感受野到图片的下一个区域的跨步</li>
<li><strong>padding</strong>：当感受野来到图片边界，剩下区域不够大时的填充</li>
<li>常规设置：每一个感受野有一组神经元（例如64个神经元）</li>
</ul></li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202205252105684.png"  style="zoom:67%;" /></p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202205252105098.png"  style="zoom:67%;" /></p>
<h5 id="相同特征在不同区域">相同特征在不同区域</h5>
<ul>
<li><p>给我们两张鸟的图片，它们都有鸟喙，但是它们的鸟喙在图片上的不同区域上，那对于每一个感受野来说，都需要配置一个专门的鸟喙检测的神经元吗？</p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202205252106259.png"  style="zoom:67%;" /></p></li>
<li><p>可以让所有感受野中相应的神经元来共享参数</p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202205252115474.png"  style="zoom:67%;" /></p></li>
</ul>
<h5 id="好处">好处</h5>
<ul>
<li>可以很好地处理图片
<ul>
<li>在图片中，一些重要的pattern比整张图片要小得多</li>
<li>在不同的图片中，相同的pattern会出现在图片的不同区域</li>
</ul></li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202205252103648.png"  style="zoom:80%;" /></p>
<h3 id="卷积层">卷积层</h3>
<ul>
<li>彩色：3个通道</li>
<li>黑白/灰：1个通道</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202205252122120.png"  style="zoom:80%;" /></p>
<p>假设我们的通道为1，现在我们拥有一张6*6图片，在给定的filter中，它们的值是不确定的，需要训练得到</p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206011540275.png"  style="zoom:80%;" /></p>
<p>进行卷积操作后得到的数据的结构</p>
<ul>
<li><p>列数： <span class="math display">\[
(c - c_f + 1 + padding*2)\ / \ stride
\]</span></p></li>
<li><p>行数： <span class="math display">\[
(r - r_f + 1 + padding * 2)\  / \ stride
\]</span></p></li>
<li><p>其中，</p>
<ul>
<li><p><span class="math inline">\(c\)</span>:
当前输入矩阵的列数</p></li>
<li><p><span class="math inline">\(c_f\)</span>: filter的列数</p></li>
<li><p><span class="math inline">\(r\)</span>:
当前输入矩阵的行数</p></li>
<li><p><span class="math inline">\(r_f\)</span>: filter的行数</p></li>
<li><p>padding: 指在输入矩阵外圈填充的圈数</p></li>
<li><p>stride: 指filter在移动时跨越的步数</p></li>
</ul></li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206011554197.png"  style="zoom:80%;" /></p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206011557435.png"  style="zoom:80%;" /></p>
<p>当所有的filter都对输入进行处理后，我们便获得了<strong>Feature
Map</strong>，每一个filter都是对图片的不同解读，即拓展了查看图片的角度</p>
<ul>
<li>我们可以将Feature Map投入到下一个卷积层中</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206011559173.png"  style="zoom:80%;" /></p>
<h3 id="感受野和滤波器的比较">感受野和滤波器的比较</h3>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206011602332.png"  style="zoom:67%;" /></p>
<ul>
<li>拥有不同感受野的神经元会共享相同的参数</li>
<li>每个滤波器会在整张输入图片上进行卷积操作</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206011606741.png"  style="zoom:80%;" /></p>
<h3 id="pooling">Pooling</h3>
<p>对像素进行子采样不会更改对象</p>
<ul>
<li>子采样是一种选取原始数据的子集的方法，用来减小数据的大小</li>
<li>子采样会改变数据集的拓扑，当某些部分没有被选取时，会留下拓扑上的洞</li>
</ul>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206011608798.png" /></p>
<h4 id="max-pooling">Max Pooling</h4>
<p>选取Filter中最大的值作为感受野的取值</p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206011609590.png"  style="zoom:80%;" /></p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206011609029.png"  style="zoom:80%;" /></p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206011610577.png"  style="zoom:80%;" /></p>
<h3 id="小结">小结</h3>
<ul>
<li>CNN能够捕捉局部信息，当使用CNN时，我们应该考虑我们的数据集和目标，是否适用CNN
<ul>
<li>例如Alpha
Go中，在围棋中，我们需要去考虑局部的信息，而且在这种具体的情况中，pooling并不适用，子采样会损失围棋分布的信息</li>
</ul></li>
<li>CNN在图像的放缩和旋转后，不能够正常的识别，需要我们进行数据增强(data
augmentation)</li>
</ul>
<h2 id="recurent-neural-networkrnn">Recurent Neural Network(RNN)</h2>
<h3 id="slot-filling">Slot Filling</h3>
<ul>
<li>输入一段语句，给出填空的答案</li>
</ul>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206121525234.png" /></p>
<ul>
<li>可不可以使用前向网络(Feedforward network)来实现
<ul>
<li>输入单词（使用单词编码），每一个单词用一个向量来表示</li>
<li>输出单词属于某一个空的概率</li>
</ul></li>
</ul>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206121537487.png" /></p>
<ul>
<li>问题：网络无法结合上下单词，理解词汇的意义，如到达和离开的区别，只能捕捉到目的地单词
<ul>
<li>我们需要网络具有记忆的功能，能够记住前后的单词</li>
</ul></li>
</ul>
<h4 id="单词编码">单词编码</h4>
<h5 id="of-n-encoding">1-of-N encoding</h5>
<ul>
<li>向量长度为整个词库的词语数量</li>
<li>一个维度标记词库中的一个单词</li>
<li>对于某一个单词，它所在维度为1，其他维度为0</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206121530986.png"  style="zoom:67%;" /></p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206121529997.png"  style="zoom:67%;" /></p>
<h5 id="改进">改进</h5>
<h6 id="others">Others</h6>
<ul>
<li>将其他不存在词库中的单词设置为“other”</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206121531532.png"  style="zoom:67%;" /></p>
<h6 id="word-hashing">Word hashing</h6>
<ul>
<li>维度用来标记字母组合</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206121532801.png"  style="zoom:80%;" /></p>
<h3 id="rnn">RNN</h3>
<h4 id="存储前一个输入的信息">存储前一个输入的信息</h4>
<ul>
<li>将隐藏层的信息存储起来，将该信息作为输入，让网络可以学习</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206121547500.png"  style="zoom:67%;" /></p>
<h5 id="例子">例子</h5>
<ul>
<li><p>假设所有的权重为1，没有bias，激活函数都是线性的</p></li>
<li><p>输入序列： <span class="math display">\[
\begin{equation}
  \begin{bmatrix}
  1 \\
  1
  \end{bmatrix}
  \begin{bmatrix}
  1 \\
  1
  \end{bmatrix}
  \begin{bmatrix}
  2 \\
  2
  \end{bmatrix}
\end{equation}
\]</span></p></li>
<li><p>初始的存储值为0</p></li>
</ul>
<p>第一次输入: <span class="math display">\[
\begin{bmatrix}
    1 \\
    1
\end{bmatrix}
\]</span> <img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206121554397.png" /></p>
<ul>
<li><p>两个存储值都会变为2</p></li>
<li><p>输出为 <span class="math display">\[
\begin{bmatrix}
  4 \\
  4
\end{bmatrix}
\]</span></p></li>
</ul>
<p>第二次输入： <span class="math display">\[
\begin{bmatrix}
    1 \\
    1
\end{bmatrix}
\]</span> <img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206121557234.png" /></p>
<ul>
<li>两个存储值会变为6</li>
<li>输出为</li>
</ul>
<p><span class="math display">\[
\begin{bmatrix}
    12 \\
    12
\end{bmatrix}
\]</span></p>
<h5 id="小结-1">小结</h5>
<ul>
<li>改变输入序列的顺序，会改变输出</li>
<li>反复使用这样的结构</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206121559818.png"  style="zoom:80%;" /></p>
<p>如上图所示，当前输入，可以获得前一个输入的信息，可以简单区分出一些不同</p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206121604746.png"  style="zoom:80%;" /></p>
<ul>
<li>可以将网络做深</li>
</ul>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206121605857.png" /></p>
<h5 id="类别">类别</h5>
<ul>
<li>Elman Network：传递前一个输入的隐藏层信息</li>
<li>Jordan Network：传递前一个输出的信息</li>
</ul>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206121607014.png" /></p>
<h4 id="双向rnn">双向RNN</h4>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206121608475.png" /></p>
<p>将前向输入和逆向输入相同位置上的隐藏信息拼合到一个，存储当前单词前后的信息</p>
<h4 id="lstm">LSTM</h4>
<h5 id="结构">结构</h5>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206121617309.png" /></p>
<ul>
<li><p>由4个部分组成</p>
<ul>
<li>Input Gate：由信号控制是否接收输入</li>
<li>Memory Cell：存储记忆的信息</li>
<li>Forget Gate：由信号控制是否清除现在存储的信息</li>
<li>Output Gate：由信号控制是否输出</li>
</ul></li>
<li><p>一共有4个输入，1个输出</p>
<ul>
<li>输入：3个signal，1个正常输入</li>
</ul></li>
</ul>
<h5 id="计算过程">计算过程</h5>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206121626766.png" /></p>
<ul>
<li>激活函数f输出为0到1，可以用来控制是否接收信息</li>
<li>输入<span class="math inline">\(z\)</span>和<span
class="math inline">\(z_i\)</span>，经过激活函数得到<span
class="math inline">\(g(z)\)</span>和<span
class="math inline">\(f(z_i)\)</span>，将两者相乘，即为<span
class="math inline">\(g(z)f(z_i)\)</span>
<ul>
<li>这一步用来控制输入</li>
</ul></li>
<li>输入<span class="math inline">\(z_f\)</span>，经过激活函数得到<span
class="math inline">\(f(z_f)\)</span>，与Memory
Cell中存储的信息c进行相乘，即为<span
class="math inline">\(cf(z_f)\)</span>
<ul>
<li>这一步用来控制是否要清楚当前信息c</li>
</ul></li>
<li>将前两步获得的数据相加获得新的存储信息<span
class="math inline">\(c^\prime\)</span></li>
</ul>
<p><span class="math display">\[
c^\prime = g(z)f(z_i) + cf(z_f)
\]</span></p>
<ul>
<li><p>输入<span class="math inline">\(c^\prime\)</span>和<span
class="math inline">\(z_o\)</span>，经过激活函数得到<span
class="math inline">\(h(c^\prime)\)</span>和<span
class="math inline">\(f(z_o)\)</span>，将两者相乘，得到输出a，即 <span
class="math display">\[
a = h(c^\prime)f(z_0)
\]</span></p>
<ul>
<li>这一步用来控制输出</li>
</ul></li>
</ul>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206121852288.png" /></p>
<p>在RNN网络架构中，一般用LSTM代替神经元</p>
<h5 id="缺点">缺点</h5>
<ul>
<li><p>参数过多</p>
<ul>
<li>每一个LSTM都需要4个输入，需要4倍的参数*LSTM数目</li>
</ul>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206121908775.png" /></p>
<ul>
<li>解决方案：利用当前输入，生成4个向量，所有的LSTM使用对应位置上的同一向量</li>
</ul>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206121918618.png" /></p>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206121921657.png" /></p>
<ul>
<li>可以将上一个LSTM网络中的c拼合输入中</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206121927804.png"  style="zoom: 80%;" /></p></li>
</ul>
<h5 id="多层lstm">多层LSTM</h5>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206121933005.png"  style="zoom: 67%;" /></p>
<h5 id="学习目标">学习目标</h5>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206121935566.png"  style="zoom:67%;" /></p>
<ul>
<li>BPTT（Backpropagation through time）</li>
</ul>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206121937184.png" /></p>
<h4 id="rnn训练较为困难">RNN训练较为困难</h4>
<ul>
<li>RNN参数的损失曲面十分陡峭</li>
<li>可以采用clip，剪切掉超过某一范围的参数，强制在一定范围内</li>
</ul>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206121939602.png" /></p>
<ul>
<li>损失曲面会抖动严重的原因</li>
</ul>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206121943377.png" /></p>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206121945280.png" /></p>
<p>参数的细微改变，会导致后面的输出发生巨大变化（梯度爆炸），或者一直为0（梯度消失），学习率无法调节</p>
<h4 id="lstm的优势">LSTM的优势</h4>
<ul>
<li>可以解决梯度消失的问题（不是梯度爆炸）
<ul>
<li>fotget gate关闭可以消除前面记录信息的影响，摆脱梯度消失</li>
</ul></li>
<li>记录的信息和输入可以拼合</li>
</ul>
<h4 id="rnn的应用场景">RNN的应用场景</h4>
<h5 id="many-to-one">Many to one</h5>
<h6 id="sentiment-analysis">Sentiment Analysis</h6>
<ul>
<li>语句分析，将其分类</li>
<li>输入：向量序列</li>
<li>输出：向量标签</li>
</ul>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206121957183.png" /></p>
<h6 id="key-term-extraction">Key Term Extraction</h6>
<ul>
<li>关键词的提取</li>
<li>输入：向量序列</li>
<li>输出：一个向量</li>
</ul>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206122000086.png" /></p>
<h5 id="many-to-many输出序列较短">Many to Many（输出序列较短）</h5>
<p>输入和输出都是序列，输出序列较短</p>
<h6 id="speech-recognition">Speech Recognition</h6>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206122005975.png" /></p>
<p>好棒棒这种叠词，可能无法识别</p>
<p>Connectionist Temporal Classification (CTC)</p>
<ul>
<li>加入了额外的符号<span
class="math inline">\(\phi\)</span>来代表空</li>
</ul>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206122008246.png" /></p>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206122010241.png" /></p>
<ul>
<li>CTC是识别每一个字母的</li>
</ul>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206122010310.png" /></p>
<h5 id="many-to-many-没有限制">Many to Many （没有限制）</h5>
<p>输入和输出都是序列，且长度没有限制，可以不一样 -&gt; Sequence to
sequence learning</p>
<h6 id="machine-translation">Machine Translation</h6>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206150959111.png"  style="zoom:80%;" /></p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151009677.png"  style="zoom:80%;" /></p>
<h5 id="beyond-sequence">Beyond Sequence</h5>
<h6 id="syntactic-parsing">Syntactic parsing</h6>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151011308.png"  style="zoom:80%;" /></p>
<h5 id="sequence-to-sequence-auto-encoder-text">Sequence-to-sequence
Auto-encoder Text</h5>
<p>要理解一个句子的意思，单词的顺序不可以忽略</p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151014571.png"  style="zoom:80%;" /></p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151015496.png"  style="zoom: 80%;" /></p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151016619.png"  style="zoom:80%;" /></p>
<h5 id="sequence-to-sequence-auto-encoder-speech">Sequence-to-sequence
Auto-encoder Speech</h5>
<ul>
<li>不定长度的序列的降维</li>
<li>发音相近的词语转换为向量后，会聚集在一定区域</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151022522.png"  style="zoom:80%;" /></p>
<p>音频归档分为可变长度的音频段，然后可以对语音进行检索</p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151026948.png"  style="zoom:80%;" /></p>
<p>将语音片段转换为向量后，我们希望向量能够表示这个语音片段</p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151029985.png"  style="zoom:80%;" /></p>
<p>Encoder和Decoder是联合训练的</p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151030015.png"  style="zoom:80%;" /></p>
<h4 id="attention-based-model">Attention-based Model</h4>
<ul>
<li>DNN/RNN可以通过Reading Head
Controller在Memory中找到自己想要的相关信息</li>
<li>就像人类的大脑一样，可以去记忆中去搜索相关的知识</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151034669.png"  style="zoom:80%;" /></p>
<ul>
<li>DNN/RNN同时也可以写入Memory</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151035798.png"  style="zoom:80%;" /></p>
<h5 id="应用">应用</h5>
<h6 id="reading-comprehension">Reading Comprehension</h6>
<ul>
<li>对文本进行语义分析，每一个句子转换为一个向量</li>
<li>DNN/RNN读取向量信息</li>
</ul>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151242792.png" /></p>
<h6 id="visual-question-answering">Visual Question Answering</h6>
<ul>
<li>输入一张图片和一个问题，输出问题的答案</li>
</ul>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151244411.png" /></p>
<ul>
<li>利用CNN将图片的每一个区域转换为一个向量</li>
</ul>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151245876.png" /></p>
<h6 id="speech-question-answering">Speech Question Answering</h6>
<p>听力考试</p>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151251480.png" /></p>
<h4 id="rnn和structured-learning的比较">RNN和Structured
Learning的比较</h4>
<p>RNN，LSTM</p>
<ul>
<li>非双向的RNN不能够考虑整个序列</li>
<li>Cost和eroor总是相关的</li>
<li>deep</li>
</ul>
<p>HMM，CRF，Structured Perceptron/SVM</p>
<ul>
<li>使用了Viterbi，考虑了整个序列
<ul>
<li>但双向的RNN也可考虑</li>
</ul></li>
<li>可以明确考虑标签依赖关系</li>
<li>Cost是error的下界</li>
</ul>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151300653.png" /></p>
<h5 id="一起使用">一起使用</h5>
<h6 id="speech-recognition-cnnlstmdnn-hmm">Speech Recognition:
CNN/LSTM/DNN + HMM</h6>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151308161.png"  style="zoom:80%;" /></p>
<h6 id="semantic-tagging-bi-directional-lstm-crfstructured-svm">Semantic
Tagging: Bi-directional LSTM + CRF/Structured SVM</h6>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151310882.png" /></p>
<h2 id="self-attention">Self-Attention</h2>
<h4 id="输入">输入</h4>
<ul>
<li>输入是一个向量，经过模型后输出一个数字或者类别</li>
<li>输入时一组向量，经过模型后输出一组数字或者类别</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206011932332.png"  style="zoom:80%;" /></p>
<p><strong>输入时是一组向量</strong></p>
<ul>
<li>语句</li>
<li>语音</li>
<li>图（如关系图）</li>
</ul>
<p><strong>语句</strong></p>
<p>如一句话“this is a
cat”，我们需要对数据集进行编码处理，以便识别各个单词</p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206011936886.png"  style="zoom:50%;" /></p>
<ul>
<li>One-hot Encoding 一键编码
<ul>
<li>有多少个单词，就构建一个多长的向量</li>
<li>这样做的后果是模型不知道单词之间的关系，它们是割裂的</li>
</ul></li>
<li>Word Embedding 词嵌入
<ul>
<li>对词语进行编码处理，相似意义的词会聚集</li>
</ul></li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206011933411.png"  style="zoom:80%;" /></p>
<p><strong>语音</strong></p>
<p>进行加窗处理</p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206011939047.png"  style="zoom:80%;" /></p>
<p><strong>图</strong></p>
<p>将每一个节点都视为一个向量</p>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206011940622.png" /></p>
<h4 id="输出">输出</h4>
<ul>
<li>每一个向量都有一个标签</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206011942420.png"  style="zoom:80%;" /></p>
<p>例子</p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206011941403.png"  style="zoom:80%;" /></p>
<ul>
<li>整个序列有一个标签</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206011942658.png"  style="zoom:80%;" /></p>
<p>例子</p>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206011943264.png" /></p>
<ul>
<li>模型自己决定有多少个标签（seq2seq）</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206011946215.png"  style="zoom:80%;" /></p>
<h4 id="思考">思考</h4>
<ul>
<li>对于一段文本，我们需要考虑前后文，如判断"I saw a
saw"中各个单词的词性
<ul>
<li>对于前后文问题，我们可以将一个窗口内的单词都输入一个全连接层中，但是这样较前面的和较后面的很难一起考虑</li>
<li>考虑整个句子，可以将整个句子丢入一个全连接层中，但是这样模型会变得很复杂</li>
</ul></li>
</ul>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206012004752.png" /></p>
<ul>
<li>可以使用自注意力机制</li>
</ul>
<h3 id="自注意力">自注意力</h3>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206012006615.png"  style="zoom: 67%;" /></p>
<h4 id="实现">实现</h4>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206012007564.png"  style="zoom: 67%;" /></p>
<h5
id="考虑当前向量和序列中其他向量的关系">考虑当前向量和序列中其他向量的关系</h5>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206012008036.png"  style="zoom:67%;" /></p>
<h5
id="可以对两个要关联的向量进行某种运算">可以对两个要关联的向量进行某种运算</h5>
<ul>
<li><strong>Dot-product</strong></li>
<li>Additive</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206012010332.png"  style="zoom:67%;" /></p>
<p>对于当前向量，计算出一个query向量，对于其他的向量，各有一个key向量，可以分别计算得出注意力分数(attention
score)</p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206012012018.png"  style="zoom: 67%;" /></p>
<p>可以对注意力分数(attention score)进行softmax运算</p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206012013572.png"  style="zoom:50%;" /></p>
<p>然后基于注意力分数(attention score)，利用value向量，提取出信息</p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206012015446.png"  style="zoom:50%;" /></p>
<p>对于所有的向量，平行地进行运算</p>
<p><img src="C:/Users/12554/AppData/Roaming/Typora/typora-user-images/image-20220601201612541.png"  style="zoom:50%;" /></p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206012016199.png"  style="zoom:67%;" /></p>
<h5
id="对于这样相同的运算我们可以将这些向量拼合成矩阵进行矩阵运算加快运算">对于这样相同的运算，我们可以将这些向量拼合成矩阵，进行矩阵运算，加快运算</h5>
<ul>
<li>q，k，v的获取
<ul>
<li>query：<span class="math inline">\(Q = W^q I\)</span></li>
<li>key：<span class="math inline">\(K = W^k I\)</span></li>
<li>value：<span class="math inline">\(V = W^vI\)</span></li>
</ul></li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206012018858.png"  style="zoom: 80%;" /></p>
<ul>
<li>注意力分数的获取</li>
</ul>
<p><span class="math display">\[
A = K^T  Q \\
A \mathop\rightarrow^{softmax} A^`
\]</span></p>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206012020665.png" /></p>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206012024761.png" /></p>
<ul>
<li>信息的获取</li>
</ul>
<p><span class="math display">\[
O = V A^`
\]</span></p>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206012026689.png" /></p>
<ul>
<li>小结</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206012027497.png"  style="zoom:67%;" /></p>
<h3 id="multi-head-self-attention">Multi-head Self-attention</h3>
<ul>
<li>多头注意力机制</li>
<li>可以探索不同类型之间的联系</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206012031288.png"  style="zoom: 67%;" /></p>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206012032401.png" /></p>
<h3 id="positional-encoding">Positional Encoding</h3>
<ul>
<li>在注意力机制中，序列输入后，模型没有对于位置的信息，不同距离的两个向量对于模型来说是一样的</li>
<li>我们可以在向量中加入位置信息，每一个位置有一个独一无二的向量<span
class="math inline">\(e^i\)</span>
<ul>
<li>手工制作</li>
<li>从数据中学习</li>
</ul></li>
</ul>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206012039598.png" /></p>
<h3 id="自注意力的应用">自注意力的应用</h3>
<ul>
<li><p>NLP 自然语言处理</p></li>
<li><p>语音处理：Truncated Self-attention</p></li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206012043669.png"  style="zoom:67%;" /></p>
<ul>
<li>图像</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206012047516.png"  style="zoom: 67%;" /></p>
<ul>
<li>Self-attention GAN</li>
</ul>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206012047338.png" /></p>
<ul>
<li>DEtection Transformer(DETR)</li>
</ul>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206012048033.png" /></p>
<h3 id="self-attention和cnn的比较">Self-attention和CNN的比较</h3>
<ul>
<li>CNN是简化版的self-attention
<ul>
<li>CNN是可以只关注一个感受野的self-attention</li>
</ul></li>
<li>self-attention是复杂版的CNN
<ul>
<li>Self-attention是具有可学习感受野的CNN</li>
</ul></li>
</ul>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206012100697.png" /></p>
<ul>
<li>Self-attention适合更多的数据，而CNN适合比较少的数据</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206012102106.png"  style="zoom:80%;" /></p>
<h3 id="self-attention和rnn的比较">Self-attention和RNN的比较</h3>
<p>RNN不可以平行处理，而自注意力可以</p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206012103414.png"  style="zoom:80%;" /></p>
<h3 id="self-attention-for-graph">Self-attention for Graph</h3>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206012104168.png"  style="zoom:80%;" /></p>
<h3 id="transformer">Transformer</h3>
<h4 id="seq2seq">Seq2seq</h4>
<ul>
<li>对于序列到序列的模型，我们输入一段序列，模型会输出一段序列，且输出序列的长度取决于模型
<ul>
<li>语音识别(Speech Recognition)</li>
<li>机器翻译(Machine Translation)</li>
<li>语音翻译(Speech Translation)</li>
<li>文本转语音合成器(Text-to-Speech(TTS) Synthesis)</li>
<li>Seq2seq for Chatbot</li>
</ul></li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206201016079.png"  style="zoom:80%;" /></p>
<ul>
<li><p>其他的一些应用</p>
<ul>
<li>Seq2seq for Syntactic Parsing</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206201020094.png"  style="zoom:80%;" /></p>
<ul>
<li>Seq2seq for Multi-label Classification</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206201021238.png"  style="zoom:80%;" /></p>
<ul>
<li>Seq2seq for Object Detection</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206201021586.png"  style="zoom:80%;" /></p></li>
</ul>
<h4 id="transformer-1">Transformer</h4>
<p>Encoder-Decoder架构</p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206201023469.png"  style="zoom:80%;" /></p>
<p>结构</p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206201026730.png"  style="zoom: 80%;" /></p>
<h5 id="encoder">Encoder</h5>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206201027152.png"  style="zoom:80%;" /></p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206201031355.png"  style="zoom:80%;" /></p>
<p>Layer Norm的调整</p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206201031877.png"  style="zoom:80%;" /></p>
<h5 id="decoder">Decoder</h5>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206201038711.png"  style="zoom: 67%;" /></p>
<p>Transformer的Decoder是一个自回归的Decoder（Autoregressive
Decoder）</p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206201036338.png"  style="zoom:80%;" /></p>
<h5 id="masked-self-attention">Masked Self-attention</h5>
<ul>
<li>在解码器中，第一个注意力机制是一个掩码的自注意力</li>
<li>这是因为解码器需要输出一个序列，它需要在不知道后面的信息的情况下，根据前面的信息，来预测当前的输出</li>
<li>所以我们在训练时，需要将后面的序列盖住</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206201042918.png"  style="zoom:80%;" /></p>
<ul>
<li>在训练时，我们给解码器的输入都是正确答案，帮助其完成训练</li>
</ul>
<p><strong>停止符号</strong></p>
<p>为了让输出停止，我们需要定义一个停止符号，提示解码器停止输出</p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206201043748.png"  style="zoom:80%;" /></p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206201044384.png"  style="zoom:80%;" /></p>
<h5 id="nat-非自回归">NAT 非自回归</h5>
<ul>
<li>AT需要前面的输出信息，才能给出当前的输出</li>
<li>NAT可以并行地给出输出，速度比AT要快，生成更加稳定</li>
</ul>
<p>NAT解码器如何决定输出的长度</p>
<ul>
<li>预测输出序列的长度(predictor)</li>
<li>输出一个固定长度的长序列，忽略终止符END后面的Token</li>
</ul>
<h5 id="cross-attention">Cross attention</h5>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206201057230.png"  style="zoom:80%;" /></p>
<p>Cross attention部分会将Encoder的最后一个输出转换为Decoder部分的<span
class="math inline">\(k\)</span>和<span
class="math inline">\(v\)</span>矩阵</p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206201416702.png"  style="zoom:80%;" /></p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206201421768.png"  style="zoom:80%;" /></p>
<h6 id="不同的连接方式">不同的连接方式</h6>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206201422724.png"   /></p>
<h4 id="训练transformer">训练Transformer</h4>
<ul>
<li>Encoder和Decoder联合训练</li>
<li>使用softmax和cross entropy进行训练</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206201437245.png"  style="zoom:80%;" /></p>
<h5 id="teacher-forcing模式">Teacher Forcing模式</h5>
<p>将输出数据作为Decoder的输入，使其向正确输出靠近</p>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206201439938.png" /></p>
<h4 id="tips">Tips</h4>
<h5 id="copy-machanism-复制机制">Copy Machanism 复制机制</h5>
<p>将一些不太需要翻译的人名、地名等直接复制</p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206201441878.png"  style="zoom:80%;" /></p>
<h5 id="guided-attention">Guided Attention</h5>
<ul>
<li><p>Monotonic Attention</p></li>
<li><p>Location-aware attention</p></li>
<li><p>在一些任务中，输入和输出是单调排列的，顺序关系不可改变</p></li>
<li><p>例如下面的序列中，在输出时，第一个输出中，Attention关注的是后面的序列</p></li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206201450843.png"  style="zoom:80%;" /></p>
<h5 id="beam-search">Beam Search</h5>
<ul>
<li>在Attention
Score中，模型会选择当前分数最高的，但是多次选择后，综合起来，不一定是最好的选择</li>
<li>Beam Search会综合考虑全局的分数，选择最好的结果</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206201453733.png"  style="zoom: 80%;" /></p>
<p>但是最好的选择不一定会产生很好的结果</p>
<h5 id="sampling">Sampling</h5>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206201456221.png" /></p>
<p>在一些任务中，生成序列时，Decoder需要一些噪声(Randomness)，sample是指从某些分布中sample中出来的噪声</p>
<h6 id="scheduled-sampling">Scheduled Sampling</h6>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206201508376.png"  style="zoom:80%;" /></p>
<p>采样由Decoder决定</p>
<h5 id="optimizing-evaluation-metrics">Optimizing Evaluation
Metrics</h5>
<p>优化方案的选择</p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206201501785.png"  style="zoom:80%;" /></p>
<ul>
<li>Cross Entropy</li>
<li>BLEU score</li>
<li>Reinforcement learning</li>
</ul>
<h3 id="各种各样的attention">各种各样的Attention</h3>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151319663.png"  style="zoom:80%;" /></p>
<h4 id="自注意力机制的运作">自注意力机制的运作</h4>
<ul>
<li>利用Query和Key构造一个Attention Matrix</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151320098.png"  style="zoom:80%;" /></p>
<ul>
<li>自注意力只是一个大型网络的一个模块</li>
<li>当序列的长度N足够大的时候，自注意力在计算占主导</li>
<li>通常用于图像处理</li>
</ul>
<h4
id="人工干预来跳过一些attention-matrix的计算">人工干预来跳过一些Attention
Matrix的计算</h4>
<h5 id="local-attentiontruncated-attention">Local Attention/Truncated
Attention</h5>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151328102.png"  style="zoom:80%;" /></p>
<ul>
<li>只计算标记的部位，其他位置设置为0</li>
<li>跟CNN有些相似</li>
</ul>
<h5 id="stride-attention">Stride Attention</h5>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151330114.png"  style="zoom:80%;" /></p>
<ul>
<li>选中从自身向左右两边跨越规定的步数的部位，进行计算</li>
</ul>
<h5 id="global-attention">Global Attention</h5>
<ul>
<li>在原来的序列中添加special token
<ul>
<li>关注(attend)每一个token，可以收集全局信息</li>
<li>被每一个token关注(attend)，知道全局信息</li>
</ul></li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151341781.png"  style="zoom:67%;" /></p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151353367.png"  style="zoom:80%;" /></p>
<p>图中前两个是special token</p>
<h5 id="小结-2">小结</h5>
<ul>
<li>可以在一个模型中同时使用不同的Attention
<ul>
<li>不同的head可以使用不同的模式</li>
</ul></li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151355615.png"  style="zoom:80%;" /></p>
<h4
id="关注一些matrix中关键的部分critical-parts">关注一些Matrix中关键的部分(Critical
Parts)</h4>
<ul>
<li>小数值直接设置为0</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151415355.png"  style="zoom:80%;" /></p>
<ul>
<li>如何快速估计注意力权重较小的部分</li>
</ul>
<h5 id="clustering">Clustering</h5>
<ul>
<li>Reformer</li>
<li>Routing Transformer</li>
</ul>
<h6 id="步骤1">步骤1</h6>
<p>基于相似度对query和key进行聚类标注</p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151435498.png"  style="zoom:80%;" /></p>
<h6 id="步骤2">步骤2</h6>
<ul>
<li>相同类别的才计算attention weight</li>
<li>不同类别的设置为0</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151436784.png"  style="zoom:80%;" /></p>
<h5 id="learnable-patterns">Learnable Patterns</h5>
<p>通过学习来获取需要计算的部位</p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151443520.png"  style="zoom:80%;" /></p>
<h5 id="不需要完整的attention-matrix">不需要完整的Attention matrix</h5>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151446205.png"  style="zoom:80%;" /></p>
<ul>
<li>可以减少key和value的长度</li>
<li>query的长度不变，因为输出长度需要不变</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151453211.png"  style="zoom:80%;" /></p>
<h6 id="使用卷积">使用卷积</h6>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151457166.png"  style="zoom:80%;" /></p>
<h6 id="使用参数矩阵相乘">使用参数矩阵相乘</h6>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151458612.png"  style="zoom:80%;" /></p>
<h4 id="注意力机制计算">注意力机制计算</h4>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151500280.png"  style="zoom:80%;" /></p>
<ul>
<li>利用输入I获取Q、K、V进行运算，得到输出O</li>
<li>为加快运算，采用矩阵计算的形式</li>
</ul>
<h5 id="计算方式的不同">计算方式的不同</h5>
<ul>
<li><span class="math inline">\(K^TQ\)</span>先计算</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151505401.png"  style="zoom:80%;" /></p>
<ul>
<li><span class="math inline">\(VK^T\)</span>先计算</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151506681.png"  style="zoom:80%;" /></p>
<p>三个矩阵计算顺序不同，进行乘法次数不同，结果相同</p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206151608002.png"  style="zoom:80%;" /></p>
<p>而Q、K、V三个矩阵的运算中，</p>
<p>正常进行计算，会进行<span
class="math inline">\((d+d^\prime)N^2\)</span>次乘法运算</p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206191607097.png" style="zoom:80%;" /></p>
<p>先计算<span class="math inline">\(V K^T\)</span>的话，会计算<span
class="math inline">\(2d^\prime
dN\)</span>次乘法运算，小于正常运算次数，序列长度N是大于维度d的</p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206191609388.png"  style="zoom:80%;" /></p>
<h5 id="注意力计算的变换">注意力计算的变换</h5>
<p>我们利用当前向量的<span
class="math inline">\(q\)</span>矩阵，来与自身和其他向量的<span
class="math inline">\(k\)</span>矩阵进行运算，再经过<span
class="math inline">\(softmax\)</span>运算，得到<span
class="math inline">\(\alpha^{\prime}\)</span>，即 <span
class="math display">\[
\alpha^{\prime}_{1,i} = \sum_{i=1}^{N}\frac{exp(q^1\cdot
k^i)}{\sum^N_{j=1}exp(q^1\cdot k^j)}
\]</span> 这里以<span class="math inline">\(a^1\)</span>向量(<span
class="math inline">\(q^1\)</span>矩阵)为当前向量。</p>
<p>再将得到的<span
class="math inline">\(\alpha^{\prime}\)</span>与对应的<span
class="math inline">\(v\)</span>矩阵进行运算，合成起来得到<span
class="math inline">\(b^1\)</span>，即 <span class="math display">\[
b^1 = \sum_{i=1}^{N}\alpha^{\prime}_{1,i}v^i =
\sum_{i=1}^{N}\frac{exp(q^1\cdot k^i)}{\sum^N_{j=1}exp(q^1\cdot k^j)}v^i
\]</span>
<img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206191622844.png"  style="zoom:80%;" /></p>
<p>在这里，我们可以对<span
class="math inline">\(softmax\)</span>的计算进行改变 <span
class="math display">\[
exp(q\cdot k) \approx \phi(q) \cdot \phi(k)
\]</span> 这里的<span
class="math inline">\(\phi\)</span>是一个变换，那么<span
class="math inline">\(b\)</span>的计算可以变换为 <span
class="math display">\[
\begin{equation}
\begin{aligned}
b^1 = \sum_{i=1}^{N}\alpha^{\prime}_{1,i}v^i
&amp;=\sum_{i=1}^{N}\frac{exp(q^1\cdot k^i)}{\sum^N_{j=1}exp(q^1\cdot
k^j)}v^i \\ \\
&amp;=
\sum^N_{i=1}\frac{\phi(q^1)\cdot\phi(k^i)}{\sum^N_{j=1}\phi(q^1)\cdot\phi(k^j)}v^i
\\ \\
&amp;=
\frac{\sum^N_{i=1}\phi(q^1)\cdot\phi(k^i)v^i}{\sum^N_{j=1}\phi(q^1)\cdot\phi(k^j)}
\end{aligned}
\end{equation}
\]</span> 而分母项可以进一步进行调整 <span class="math display">\[
\begin{equation}
\sum^N_{j=1}\phi(q^1)\cdot\phi(k^j) = \phi(q^1)
\cdot\sum^{N}_{j=1}\phi(k^j)
\end{equation}
\]</span>
<img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206191639053.png"  style="zoom:67%;" /></p>
<p>那么<span class="math inline">\(b^1\)</span>的计算进一步调整</p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206191640161.png"  style="zoom:80%;" /></p>
<p>我们可以利用<span class="math inline">\(k\)</span>和<span
class="math inline">\(v\)</span>提前计算好部分值，需要时直接调用即可</p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206191642452.png"  style="zoom:80%;" /></p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206191816815.png"  style="zoom:80%;" /></p>
<p>这样子，有一部分不用重复进行计算</p>
<p><img
src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206191819653.png" /></p>
<p>这样利用<span class="math inline">\(\phi(k)\)</span>和<span
class="math inline">\(v\)</span>计算出来的M组向量相当于有M组模板，<span
class="math inline">\(\phi(q)\)</span>与之相乘是在进行选择。</p>
<h4 id="通过学习来构造attention-matrix">通过学习来构造Attention
Matrix</h4>
<p>注意力机制通过<span class="math inline">\(q\)</span>和<span
class="math inline">\(k\)</span>来计算Attention
Matrix，但是我们可以将整个矩阵是为网络的参数</p>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206191836293.png"  style="zoom:80%;" /></p>
<h4 id="小结-3">小结</h4>
<p><img src="https://fastly.jsdelivr.net/gh/Ye2222/blogImage@main/images2/202206191837325.png"  style="zoom:80%;" /></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://ye2222.github.io">GuoYB</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://ye2222.github.io/posts/48150/">https://ye2222.github.io/posts/48150/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://ye2222.github.io" target="_blank">Yeの博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/DeepLearning/">DeepLearning</a></div><div class="post_share"><div class="social-share" data-image="/img/avatar.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/32204/" title="趣谈Linux1"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">趣谈Linux1</div></div></a></div><div class="next-post pull-right"><a href="/posts/54556/" title="originL4-LabTalk"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">originL4-LabTalk</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/32151/" title="李宏毅深度学习L1"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-05-07</div><div class="title">李宏毅深度学习L1</div></div></a></div><div><a href="/posts/31959/" title="李宏毅深度学习L2"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-05-07</div><div class="title">李宏毅深度学习L2</div></div></a></div><div><a href="/posts/32343/" title="李宏毅深度学习L4"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-06-20</div><div class="title">李宏毅深度学习L4</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">GuoYB</div><div class="author-info__description">欢迎欢迎</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">105</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">9</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">32</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ye2222"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/ye2222" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:12554804@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a><a class="social-icon" href="https://t.me/mumukin" target="_blank" title="Telegram"><i class="fab fa-telegram" style="color: #1E3050;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Hahaha~</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#convolutional-neural-network-cnn"><span class="toc-number">1.</span> <span class="toc-text">Convolutional Neural Network
（CNN）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB"><span class="toc-number">1.1.</span> <span class="toc-text">图片分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%84%9F%E5%8F%97%E9%87%8E"><span class="toc-number">1.2.</span> <span class="toc-text">感受野</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%9B%B8%E5%90%8C%E7%89%B9%E5%BE%81%E5%9C%A8%E4%B8%8D%E5%90%8C%E5%8C%BA%E5%9F%9F"><span class="toc-number">1.2.0.1.</span> <span class="toc-text">相同特征在不同区域</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A5%BD%E5%A4%84"><span class="toc-number">1.2.0.2.</span> <span class="toc-text">好处</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-number">1.3.</span> <span class="toc-text">卷积层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%84%9F%E5%8F%97%E9%87%8E%E5%92%8C%E6%BB%A4%E6%B3%A2%E5%99%A8%E7%9A%84%E6%AF%94%E8%BE%83"><span class="toc-number">1.4.</span> <span class="toc-text">感受野和滤波器的比较</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pooling"><span class="toc-number">1.5.</span> <span class="toc-text">Pooling</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#max-pooling"><span class="toc-number">1.5.1.</span> <span class="toc-text">Max Pooling</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-number">1.6.</span> <span class="toc-text">小结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#recurent-neural-networkrnn"><span class="toc-number">2.</span> <span class="toc-text">Recurent Neural Network(RNN)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#slot-filling"><span class="toc-number">2.1.</span> <span class="toc-text">Slot Filling</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%95%E8%AF%8D%E7%BC%96%E7%A0%81"><span class="toc-number">2.1.1.</span> <span class="toc-text">单词编码</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#of-n-encoding"><span class="toc-number">2.1.1.1.</span> <span class="toc-text">1-of-N encoding</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%94%B9%E8%BF%9B"><span class="toc-number">2.1.1.2.</span> <span class="toc-text">改进</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#others"><span class="toc-number">2.1.1.2.1.</span> <span class="toc-text">Others</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#word-hashing"><span class="toc-number">2.1.1.2.2.</span> <span class="toc-text">Word hashing</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rnn"><span class="toc-number">2.2.</span> <span class="toc-text">RNN</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AD%98%E5%82%A8%E5%89%8D%E4%B8%80%E4%B8%AA%E8%BE%93%E5%85%A5%E7%9A%84%E4%BF%A1%E6%81%AF"><span class="toc-number">2.2.1.</span> <span class="toc-text">存储前一个输入的信息</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BE%8B%E5%AD%90"><span class="toc-number">2.2.1.1.</span> <span class="toc-text">例子</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93-1"><span class="toc-number">2.2.1.2.</span> <span class="toc-text">小结</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%B1%BB%E5%88%AB"><span class="toc-number">2.2.1.3.</span> <span class="toc-text">类别</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%8C%E5%90%91rnn"><span class="toc-number">2.2.2.</span> <span class="toc-text">双向RNN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#lstm"><span class="toc-number">2.2.3.</span> <span class="toc-text">LSTM</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BB%93%E6%9E%84"><span class="toc-number">2.2.3.1.</span> <span class="toc-text">结构</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B"><span class="toc-number">2.2.3.2.</span> <span class="toc-text">计算过程</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9"><span class="toc-number">2.2.3.3.</span> <span class="toc-text">缺点</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82lstm"><span class="toc-number">2.2.3.4.</span> <span class="toc-text">多层LSTM</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%9B%AE%E6%A0%87"><span class="toc-number">2.2.3.5.</span> <span class="toc-text">学习目标</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#rnn%E8%AE%AD%E7%BB%83%E8%BE%83%E4%B8%BA%E5%9B%B0%E9%9A%BE"><span class="toc-number">2.2.4.</span> <span class="toc-text">RNN训练较为困难</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#lstm%E7%9A%84%E4%BC%98%E5%8A%BF"><span class="toc-number">2.2.5.</span> <span class="toc-text">LSTM的优势</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#rnn%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">2.2.6.</span> <span class="toc-text">RNN的应用场景</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#many-to-one"><span class="toc-number">2.2.6.1.</span> <span class="toc-text">Many to one</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#sentiment-analysis"><span class="toc-number">2.2.6.1.1.</span> <span class="toc-text">Sentiment Analysis</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#key-term-extraction"><span class="toc-number">2.2.6.1.2.</span> <span class="toc-text">Key Term Extraction</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#many-to-many%E8%BE%93%E5%87%BA%E5%BA%8F%E5%88%97%E8%BE%83%E7%9F%AD"><span class="toc-number">2.2.6.2.</span> <span class="toc-text">Many to Many（输出序列较短）</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#speech-recognition"><span class="toc-number">2.2.6.2.1.</span> <span class="toc-text">Speech Recognition</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#many-to-many-%E6%B2%A1%E6%9C%89%E9%99%90%E5%88%B6"><span class="toc-number">2.2.6.3.</span> <span class="toc-text">Many to Many （没有限制）</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#machine-translation"><span class="toc-number">2.2.6.3.1.</span> <span class="toc-text">Machine Translation</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#beyond-sequence"><span class="toc-number">2.2.6.4.</span> <span class="toc-text">Beyond Sequence</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#syntactic-parsing"><span class="toc-number">2.2.6.4.1.</span> <span class="toc-text">Syntactic parsing</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#sequence-to-sequence-auto-encoder-text"><span class="toc-number">2.2.6.5.</span> <span class="toc-text">Sequence-to-sequence
Auto-encoder Text</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#sequence-to-sequence-auto-encoder-speech"><span class="toc-number">2.2.6.6.</span> <span class="toc-text">Sequence-to-sequence
Auto-encoder Speech</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#attention-based-model"><span class="toc-number">2.2.7.</span> <span class="toc-text">Attention-based Model</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%BA%94%E7%94%A8"><span class="toc-number">2.2.7.1.</span> <span class="toc-text">应用</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#reading-comprehension"><span class="toc-number">2.2.7.1.1.</span> <span class="toc-text">Reading Comprehension</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#visual-question-answering"><span class="toc-number">2.2.7.1.2.</span> <span class="toc-text">Visual Question Answering</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#speech-question-answering"><span class="toc-number">2.2.7.1.3.</span> <span class="toc-text">Speech Question Answering</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#rnn%E5%92%8Cstructured-learning%E7%9A%84%E6%AF%94%E8%BE%83"><span class="toc-number">2.2.8.</span> <span class="toc-text">RNN和Structured
Learning的比较</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%80%E8%B5%B7%E4%BD%BF%E7%94%A8"><span class="toc-number">2.2.8.1.</span> <span class="toc-text">一起使用</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#speech-recognition-cnnlstmdnn-hmm"><span class="toc-number">2.2.8.1.1.</span> <span class="toc-text">Speech Recognition:
CNN&#x2F;LSTM&#x2F;DNN + HMM</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#semantic-tagging-bi-directional-lstm-crfstructured-svm"><span class="toc-number">2.2.8.1.2.</span> <span class="toc-text">Semantic
Tagging: Bi-directional LSTM + CRF&#x2F;Structured SVM</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#self-attention"><span class="toc-number">3.</span> <span class="toc-text">Self-Attention</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BE%93%E5%85%A5"><span class="toc-number">3.0.1.</span> <span class="toc-text">输入</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BE%93%E5%87%BA"><span class="toc-number">3.0.2.</span> <span class="toc-text">输出</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%9D%E8%80%83"><span class="toc-number">3.0.3.</span> <span class="toc-text">思考</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">3.1.</span> <span class="toc-text">自注意力</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.1.1.</span> <span class="toc-text">实现</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%80%83%E8%99%91%E5%BD%93%E5%89%8D%E5%90%91%E9%87%8F%E5%92%8C%E5%BA%8F%E5%88%97%E4%B8%AD%E5%85%B6%E4%BB%96%E5%90%91%E9%87%8F%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-number">3.1.1.1.</span> <span class="toc-text">考虑当前向量和序列中其他向量的关系</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%8F%AF%E4%BB%A5%E5%AF%B9%E4%B8%A4%E4%B8%AA%E8%A6%81%E5%85%B3%E8%81%94%E7%9A%84%E5%90%91%E9%87%8F%E8%BF%9B%E8%A1%8C%E6%9F%90%E7%A7%8D%E8%BF%90%E7%AE%97"><span class="toc-number">3.1.1.2.</span> <span class="toc-text">可以对两个要关联的向量进行某种运算</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AF%B9%E4%BA%8E%E8%BF%99%E6%A0%B7%E7%9B%B8%E5%90%8C%E7%9A%84%E8%BF%90%E7%AE%97%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E5%B0%86%E8%BF%99%E4%BA%9B%E5%90%91%E9%87%8F%E6%8B%BC%E5%90%88%E6%88%90%E7%9F%A9%E9%98%B5%E8%BF%9B%E8%A1%8C%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E5%8A%A0%E5%BF%AB%E8%BF%90%E7%AE%97"><span class="toc-number">3.1.1.3.</span> <span class="toc-text">对于这样相同的运算，我们可以将这些向量拼合成矩阵，进行矩阵运算，加快运算</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#multi-head-self-attention"><span class="toc-number">3.2.</span> <span class="toc-text">Multi-head Self-attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#positional-encoding"><span class="toc-number">3.3.</span> <span class="toc-text">Positional Encoding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-number">3.4.</span> <span class="toc-text">自注意力的应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#self-attention%E5%92%8Ccnn%E7%9A%84%E6%AF%94%E8%BE%83"><span class="toc-number">3.5.</span> <span class="toc-text">Self-attention和CNN的比较</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#self-attention%E5%92%8Crnn%E7%9A%84%E6%AF%94%E8%BE%83"><span class="toc-number">3.6.</span> <span class="toc-text">Self-attention和RNN的比较</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#self-attention-for-graph"><span class="toc-number">3.7.</span> <span class="toc-text">Self-attention for Graph</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#transformer"><span class="toc-number">3.8.</span> <span class="toc-text">Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#seq2seq"><span class="toc-number">3.8.1.</span> <span class="toc-text">Seq2seq</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#transformer-1"><span class="toc-number">3.8.2.</span> <span class="toc-text">Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#encoder"><span class="toc-number">3.8.2.1.</span> <span class="toc-text">Encoder</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#decoder"><span class="toc-number">3.8.2.2.</span> <span class="toc-text">Decoder</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#masked-self-attention"><span class="toc-number">3.8.2.3.</span> <span class="toc-text">Masked Self-attention</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#nat-%E9%9D%9E%E8%87%AA%E5%9B%9E%E5%BD%92"><span class="toc-number">3.8.2.4.</span> <span class="toc-text">NAT 非自回归</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#cross-attention"><span class="toc-number">3.8.2.5.</span> <span class="toc-text">Cross attention</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E4%B8%8D%E5%90%8C%E7%9A%84%E8%BF%9E%E6%8E%A5%E6%96%B9%E5%BC%8F"><span class="toc-number">3.8.2.5.1.</span> <span class="toc-text">不同的连接方式</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83transformer"><span class="toc-number">3.8.3.</span> <span class="toc-text">训练Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#teacher-forcing%E6%A8%A1%E5%BC%8F"><span class="toc-number">3.8.3.1.</span> <span class="toc-text">Teacher Forcing模式</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#tips"><span class="toc-number">3.8.4.</span> <span class="toc-text">Tips</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#copy-machanism-%E5%A4%8D%E5%88%B6%E6%9C%BA%E5%88%B6"><span class="toc-number">3.8.4.1.</span> <span class="toc-text">Copy Machanism 复制机制</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#guided-attention"><span class="toc-number">3.8.4.2.</span> <span class="toc-text">Guided Attention</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#beam-search"><span class="toc-number">3.8.4.3.</span> <span class="toc-text">Beam Search</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#sampling"><span class="toc-number">3.8.4.4.</span> <span class="toc-text">Sampling</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#scheduled-sampling"><span class="toc-number">3.8.4.4.1.</span> <span class="toc-text">Scheduled Sampling</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#optimizing-evaluation-metrics"><span class="toc-number">3.8.4.5.</span> <span class="toc-text">Optimizing Evaluation
Metrics</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%84%E7%A7%8D%E5%90%84%E6%A0%B7%E7%9A%84attention"><span class="toc-number">3.9.</span> <span class="toc-text">各种各样的Attention</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E8%BF%90%E4%BD%9C"><span class="toc-number">3.9.1.</span> <span class="toc-text">自注意力机制的运作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%BA%E5%B7%A5%E5%B9%B2%E9%A2%84%E6%9D%A5%E8%B7%B3%E8%BF%87%E4%B8%80%E4%BA%9Battention-matrix%E7%9A%84%E8%AE%A1%E7%AE%97"><span class="toc-number">3.9.2.</span> <span class="toc-text">人工干预来跳过一些Attention
Matrix的计算</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#local-attentiontruncated-attention"><span class="toc-number">3.9.2.1.</span> <span class="toc-text">Local Attention&#x2F;Truncated
Attention</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#stride-attention"><span class="toc-number">3.9.2.2.</span> <span class="toc-text">Stride Attention</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#global-attention"><span class="toc-number">3.9.2.3.</span> <span class="toc-text">Global Attention</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93-2"><span class="toc-number">3.9.2.4.</span> <span class="toc-text">小结</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B3%E6%B3%A8%E4%B8%80%E4%BA%9Bmatrix%E4%B8%AD%E5%85%B3%E9%94%AE%E7%9A%84%E9%83%A8%E5%88%86critical-parts"><span class="toc-number">3.9.3.</span> <span class="toc-text">关注一些Matrix中关键的部分(Critical
Parts)</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#clustering"><span class="toc-number">3.9.3.1.</span> <span class="toc-text">Clustering</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A41"><span class="toc-number">3.9.3.1.1.</span> <span class="toc-text">步骤1</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A42"><span class="toc-number">3.9.3.1.2.</span> <span class="toc-text">步骤2</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#learnable-patterns"><span class="toc-number">3.9.3.2.</span> <span class="toc-text">Learnable Patterns</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%8D%E9%9C%80%E8%A6%81%E5%AE%8C%E6%95%B4%E7%9A%84attention-matrix"><span class="toc-number">3.9.3.3.</span> <span class="toc-text">不需要完整的Attention matrix</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E5%8D%B7%E7%A7%AF"><span class="toc-number">3.9.3.3.1.</span> <span class="toc-text">使用卷积</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E5%8F%82%E6%95%B0%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%98"><span class="toc-number">3.9.3.3.2.</span> <span class="toc-text">使用参数矩阵相乘</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E8%AE%A1%E7%AE%97"><span class="toc-number">3.9.4.</span> <span class="toc-text">注意力机制计算</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%96%B9%E5%BC%8F%E7%9A%84%E4%B8%8D%E5%90%8C"><span class="toc-number">3.9.4.1.</span> <span class="toc-text">计算方式的不同</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%AE%A1%E7%AE%97%E7%9A%84%E5%8F%98%E6%8D%A2"><span class="toc-number">3.9.4.2.</span> <span class="toc-text">注意力计算的变换</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%9A%E8%BF%87%E5%AD%A6%E4%B9%A0%E6%9D%A5%E6%9E%84%E9%80%A0attention-matrix"><span class="toc-number">3.9.5.</span> <span class="toc-text">通过学习来构造Attention
Matrix</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93-3"><span class="toc-number">3.9.6.</span> <span class="toc-text">小结</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/113347930/" title="深度学习汇总">深度学习汇总</a><time datetime="2023-05-28T07:03:20.000Z" title="发表于 2023-05-28 15:03:20">2023-05-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/12759/" title="统计学习方法L2">统计学习方法L2</a><time datetime="2023-04-15T06:23:15.000Z" title="发表于 2023-04-15 14:23:15">2023-04-15</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/48218/" title="64位系统--BootLoader">64位系统--BootLoader</a><time datetime="2023-04-13T11:36:51.000Z" title="发表于 2023-04-13 19:36:51">2023-04-13</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/7112/" title="64位系统--GNU_C">64位系统--GNU_C</a><time datetime="2023-04-13T11:31:51.000Z" title="发表于 2023-04-13 19:31:51">2023-04-13</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/12439/" title="统计学习方法L1">统计学习方法L1</a><time datetime="2023-04-11T07:56:41.000Z" title="发表于 2023-04-11 15:56:41">2023-04-11</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By GuoYB</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><a href="icp链接"><span>备案号：粤ICP备2023044853号</span></a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>